{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2eeadb",
   "metadata": {},
   "source": [
    "## ä¸€. Training Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d60978",
   "metadata": {},
   "source": [
    "### 1. the enviroment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2aa1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 3\n",
    "model_name = 'fair_single_best.pt'\n",
    "ema_model_name = 'fair_ema_best.pt'\n",
    "\n",
    "data_root = '../Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f04b9",
   "metadata": {},
   "source": [
    "### 2. import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398eb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('/home/luxiongbo/data_bak/data/code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80cc3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from base_utils.dir_util import mkdirs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0f386",
   "metadata": {},
   "source": [
    "### 3. some configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03ccdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Config.config_mnist_rot as config\n",
    "from Models.VIB_MNIST_ROT_whole_model import VariationalInformationBottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d59d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Get_Datasets import get_datasets\n",
    "from Utils.Visualize import tsne_embedding_without_images\n",
    "from Models.VIB_model import Weight_EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed6d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = config.train_args()\n",
    "dataset_args = config.dataset_args()\n",
    "model_args = config.model_args()\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a3509",
   "metadata": {},
   "source": [
    "### 4. fixed the seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6abdccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args.gpu = gpu_id\n",
    "init_seed = train_args.seed\n",
    "torch.manual_seed(init_seed)\n",
    "torch.cuda.manual_seed(init_seed)\n",
    "torch.cuda.manual_seed_all(init_seed)\n",
    "np.random.seed(init_seed)\n",
    "random.seed(init_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(train_args.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f56ee",
   "metadata": {},
   "source": [
    "### 5. loading datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9da4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist-rot dataset...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, test_55_loader, test_65_loader = get_datasets(\n",
    "    dataset_args.dataset, dataset_args.train_batch_size, dataset_args.test_batch_size, root=data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20e7f0",
   "metadata": {},
   "source": [
    "### 6. print some key vaules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcbf629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction_weight :\t 0.01\n",
      "pairwise_kl_clean_weight :\t 0.075\n",
      "pairwise_kl_noise_weight :\t 0.01\n",
      "sparse_kl_weight_clean :\t 0.1\n",
      "sparse_kl_weight_noise :\t 0.01\n",
      "sparsity_clean :\t 0.1\n",
      "sparsity_noise :\t 0.1\n",
      "num_sensitive_class :\t 5\n"
     ]
    }
   ],
   "source": [
    "parameter = {\n",
    "    \"reconstruction_weight\": model_args.reconstruction_weight,\n",
    "    \"pairwise_kl_clean_weight\": model_args.pairwise_kl_clean_weight,\n",
    "    \"pairwise_kl_noise_weight\": model_args.pairwise_kl_noise_weight,\n",
    "    \"sparse_kl_weight_clean\": model_args.sparse_kl_weight_clean,\n",
    "    \"sparse_kl_weight_noise\": model_args.sparse_kl_weight_noise,\n",
    "    \"sparsity_clean\": model_args.sparsity_clean,\n",
    "    \"sparsity_noise\": model_args.sparsity_noise,\n",
    "    \"num_sensitive_class\": dataset_args.num_sensitive_class\n",
    "}\n",
    "for k, v in parameter.items():\n",
    "    print(k, ':\\t',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03882997",
   "metadata": {},
   "source": [
    "### 7. create the saved dir..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e31624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../saved_model/Mnist-Rot/0.010_0.075_0.010/0.10_0.01_0.10_0.10/0\n"
     ]
    }
   ],
   "source": [
    "path_1 = '%.03f_%.03f_%.03f' %(model_args.reconstruction_weight, model_args.pairwise_kl_clean_weight, model_args.pairwise_kl_noise_weight)\n",
    "path_2 = '%.02f_%.02f_%.02f_%.02f' %(model_args.sparse_kl_weight_clean, model_args.sparse_kl_weight_noise, model_args.sparsity_clean, model_args.sparsity_noise)\n",
    "save_model_dir = os.path.join('../saved_model/Mnist-Rot/', path_1, path_2, str(train_args.seed))\n",
    "mkdirs(save_model_dir)\n",
    "print(save_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774ca91",
   "metadata": {},
   "source": [
    "### 8. define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa301ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_model = VariationalInformationBottleneck(\n",
    "    dataset_args.shape_data, dataset_args.num_target_class,\n",
    "    model_args.dim_embedding_clean, model_args.dim_embedding_noise,\n",
    "    model_args.channel_hidden_encoder, model_args.channel_hidden_decoder,\n",
    "    model_args.dim_hidden_classifier, parameter\n",
    ")\n",
    "\n",
    "vib_model_copy = VariationalInformationBottleneck(\n",
    "    dataset_args.shape_data, dataset_args.num_target_class,\n",
    "    model_args.dim_embedding_clean, model_args.dim_embedding_noise,\n",
    "    model_args.channel_hidden_encoder, model_args.channel_hidden_decoder,\n",
    "    model_args.dim_hidden_classifier, parameter\n",
    ")\n",
    "\n",
    "if use_cuda:\n",
    "    vib_model = vib_model.cuda()\n",
    "    vib_model_copy = vib_model_copy.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6080ff",
   "metadata": {},
   "source": [
    "### 9. optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdcfd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_model_EMA = Weight_EMA(vib_model_copy, vib_model.state_dict(), decay=0.999)\n",
    "optimizer = torch.optim.Adam(vib_model.parameters(), lr=train_args.lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "lr_scheduler = StepLR(optimizer, step_size=2, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd04c83",
   "metadata": {},
   "source": [
    "### 10. train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752efbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    total_loss_total = 0.0\n",
    "    classification_loss_total = 0.0\n",
    "    classification_sensitive_loss_total = 0.0\n",
    "    reconstruction_loss_total = 0.0\n",
    "    pairwise_kl_loss_clean_total = 0.0\n",
    "    pairwise_kl_loss_noise_total = 0.0\n",
    "    sparse_kl_loss_clean_total = 0.0\n",
    "    sparse_kl_loss_noise_total = 0.0\n",
    "\n",
    "    for iter_index, (images, labels, sensitive_labels) in enumerate(train_loader):\n",
    "        images = Variable(images.unsqueeze(dim=1).float())\n",
    "        labels = Variable(labels.long())\n",
    "        sensitive_labels = Variable(sensitive_labels.long())\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            sensitive_labels = sensitive_labels.cuda()\n",
    "        \n",
    "        (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss, pairwise_kl_loss_clean,\n",
    "         pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) = vib_model(\n",
    "            images, labels, input_sensitive_labels=sensitive_labels, num_samples=10, training=True\n",
    "        )\n",
    "        \n",
    "        total_loss_total = total_loss_total + total_loss.sum(-1)\n",
    "        classification_loss_total = classification_loss_total + classification_loss.sum(-1)\n",
    "        classification_sensitive_loss_total = classification_sensitive_loss_total + classification_sensitive_loss.sum(-1)\n",
    "        reconstruction_loss_total = reconstruction_loss_total + reconstruction_loss.sum(-1)\n",
    "        pairwise_kl_loss_clean_total = pairwise_kl_loss_clean_total + pairwise_kl_loss_clean.sum(-1)\n",
    "        pairwise_kl_loss_noise_total = pairwise_kl_loss_noise_total + pairwise_kl_loss_noise.sum(-1)\n",
    "        sparse_kl_loss_clean_total = sparse_kl_loss_clean_total + sparse_kl_loss_clean.sum(-1)\n",
    "        sparse_kl_loss_noise_total = sparse_kl_loss_noise_total + sparse_kl_loss_noise.sum(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.mean(-1).backward()\n",
    "        optimizer.step()\n",
    "        vib_model_EMA.update(vib_model.state_dict())\n",
    "    lr_scheduler.step()\n",
    "    total_loss_mean = total_loss_total / len(train_loader.dataset)\n",
    "    classification_loss_mean = classification_loss_total / len(train_loader.dataset)\n",
    "    classification_sensitive_loss_mean = classification_sensitive_loss_total / len(train_loader.dataset)\n",
    "    reconstruction_loss_mean = reconstruction_loss_total / len(train_loader.dataset)\n",
    "    pairwise_kl_loss_clean_mean = pairwise_kl_loss_clean_total / len(train_loader.dataset)\n",
    "    pairwise_kl_loss_noise_mean = pairwise_kl_loss_noise_total / len(train_loader.dataset)\n",
    "    sparse_kl_loss_clean_mean = sparse_kl_loss_clean_total / len(train_loader.dataset)\n",
    "    sparse_kl_loss_noise_mean = sparse_kl_loss_noise_total / len(train_loader.dataset)\n",
    "    return (total_loss_mean, classification_loss_mean, classification_sensitive_loss_mean,\n",
    "            reconstruction_loss_mean, pairwise_kl_loss_clean_mean, pairwise_kl_loss_noise_mean,\n",
    "            sparse_kl_loss_clean_mean, sparse_kl_loss_noise_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429586e3",
   "metadata": {},
   "source": [
    "### 11. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa023f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(epoch_index, test_dataloader, is_drawing=False):\n",
    "    vib_model.eval()\n",
    "    vib_model_EMA.model.eval()\n",
    "    avg_correct = 0.0\n",
    "    single_correct = 0.0\n",
    "\n",
    "    valid_embedding_labels = []\n",
    "    valid_embedding_sensitive_labels = []\n",
    "    valid_embedding_clean_images = []\n",
    "\n",
    "    for iter_index, data in enumerate(test_dataloader):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        images = Variable(images.unsqueeze(dim=1).float())\n",
    "        labels = Variable(labels.long())\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        avg_embedding, _, avg_classification_prob = vib_model_EMA.model(\n",
    "            images, labels, num_samples=100, training=False\n",
    "        )\n",
    "        avg_prediction = avg_classification_prob.max(1)[1]\n",
    "        avg_correct = avg_correct + torch.eq(avg_prediction, labels).float().sum()\n",
    "\n",
    "        single_embedding, _, single_classification_prob = vib_model(\n",
    "            images, labels, num_samples=100, training=False\n",
    "        )\n",
    "        single_prediction = single_classification_prob.max(1)[1]\n",
    "        single_correct = single_correct + torch.eq(single_prediction, labels).float().sum()\n",
    "\n",
    "        if is_drawing:\n",
    "            valid_embedding_labels.extend(np.asarray(labels.detach().numpy()))\n",
    "            valid_embedding_clean_images.extend(np.asarray(single_embedding.detach().numpy()))\n",
    "\n",
    "    if is_drawing:\n",
    "        tsne_embedding_without_images(images=valid_embedding_clean_images,\n",
    "                                      labels=[valid_embedding_sensitive_labels],\n",
    "                                      save_name=\"../Log/result_\" + str(epoch_index) + \"_clean.png\")\n",
    "\n",
    "    avg_correct_mean = avg_correct / len(test_dataloader.dataset)\n",
    "    single_correct_mean = single_correct / len(test_dataloader.dataset)\n",
    "    return avg_correct_mean * 100, single_correct_mean * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3a0ca",
   "metadata": {},
   "source": [
    "### 12. training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99bbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_avg_correct = 0.0\n",
    "    best_single_correct = 0.0\n",
    "    \n",
    "    epoches = 500  # train_args.max_epoch\n",
    "    for epoch_index in tqdm(range(epoches)):\n",
    "        vib_model.train()\n",
    "        vib_model_EMA.model.train()\n",
    "        \n",
    "        # avg_correct, single_correct = evaluation(epoch_index + 1, test_loader)\n",
    "        \n",
    "        (total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n",
    "         pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise) = train_one_epoch()\n",
    "        if (epoch_index + 1) % 2 == 0:\n",
    "            print('[train]Epoch: {}, total_loss: {:.4}, classification_loss: {:.4}, classification_sensitive_loss: {:.4}, '\n",
    "                  'reconstruction_loss: {:.4}, pairwise_kl_loss_clean: {:.4}, pairwise_kl_loss_noise: {:.4}, '\n",
    "                  'sparse_kl_loss_clean: {:.4}, sparse_kl_loss_noise: {:.4}'\n",
    "                  .format(epoch_index + 1, total_loss, classification_loss, classification_sensitive_loss, reconstruction_loss,\n",
    "                          pairwise_kl_loss_clean, pairwise_kl_loss_noise, sparse_kl_loss_clean, sparse_kl_loss_noise))\n",
    "\n",
    "        if (epoch_index + 1) % 2 == 0:\n",
    "            is_drawing = False\n",
    "            print('##################### test #####################')\n",
    "            avg_correct, single_correct = evaluation(epoch_index + 1, test_loader, is_drawing=is_drawing)\n",
    "        \n",
    "            if best_avg_correct <= avg_correct:\n",
    "                best_avg_correct = avg_correct\n",
    "                print(\"##################### save #####################\")\n",
    "                torch.save(vib_model_EMA.model.state_dict(), \n",
    "                           os.path.join(save_model_dir, ema_model_name))\n",
    "\n",
    "            if best_single_correct <= single_correct:\n",
    "                best_single_correct = single_correct\n",
    "                print(\"##################### save #####################\")\n",
    "                torch.save(vib_model.state_dict(),\n",
    "                           os.path.join(save_model_dir, model_name))\n",
    "\n",
    "            print('[test]Epoch: {}, avg_correct: {:.4}, best_avg_correct: {:.4}, '\n",
    "                  'single_correct: {:.4}, best_single_correct: {:.4}'\n",
    "                  .format(epoch_index + 1, avg_correct, best_avg_correct,\n",
    "                          single_correct, best_single_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73462419",
   "metadata": {},
   "source": [
    "### 13. start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86901be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–Ž                                                                                                                                                                 | 1/500 [03:58<33:06:52, 238.90s/it]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bcc34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.13.1",
   "language": "python",
   "name": "torch_1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
